suite: test model groups
tests:
  - it: should render model group deployment when enabled
    template: templates/deployment.yaml
    documentSelector:
      path: metadata.name
      value: RELEASE-NAME-inference-stack-test-vllm
    set:
      modelGroups:
        vllm-llama:
          enabled: false
        sglang-qwen:
          enabled: false
        tensorrt-llm-mistral:
          enabled: false
        test-vllm:
          enabled: true
          image:
            repository: vllm/vllm-openai
            tag: "latest"
            pullPolicy: IfNotPresent
          service:
            type: ClusterIP
            port: 8000
          containerPort: 8000
          replicaCount: 1
          modelAlias:
            - "test-model"
          modelName: "test/model"
          apiKey: ""
          additionalPorts: []
          command:
            - "--host"
            - "0.0.0.0"
          env:
            - name: TEST_VAR
              value: "test"
          livenessProbe:
            enabled: false
          readinessProbe:
            enabled: false
          resources: {}
          persistentVolumes: []
          podSecurityContext: {}
          securityContext: {}
          serviceAccount:
            create: false
            annotations: {}
          initContainers: []
          volumeMounts: []
          volumes: []
          nodeSelector: {}
          tolerations: []
          affinity: {}
          podAnnotations: {}
          imagePullSecrets: []
    asserts:
      - hasDocuments:
          count: 2
      - isKind:
          of: Deployment
      - equal:
          path: metadata.name
          value: RELEASE-NAME-inference-stack-test-vllm
      - equal:
          path: spec.template.spec.containers[0].image
          value: "vllm/vllm-openai:latest"

  - it: should not render model group deployment when disabled
    template: templates/deployment.yaml
    set:
      modelGroups:
        vllm-llama:
          enabled: false
        sglang-qwen:
          enabled: false
        tensorrt-llm-mistral:
          enabled: false
    asserts:
      - hasDocuments:
          count: 1

  - it: should render service for enabled model group
    template: templates/service.yaml
    documentSelector:
      path: metadata.name
      value: RELEASE-NAME-inference-stack-test-vllm
    set:
      modelGroups:
        vllm-llama:
          enabled: false
        sglang-qwen:
          enabled: false
        tensorrt-llm-mistral:
          enabled: false
        test-vllm:
          enabled: true
          image:
            repository: nginx
            tag: "latest"
            pullPolicy: IfNotPresent
          service:
            type: ClusterIP
            port: 8000
          containerPort: 8000
          replicaCount: 1
          modelAlias: ["test"]
          modelName: "test"
          additionalPorts: []
          persistentVolumes: []
          serviceAccount:
            create: false
          livenessProbe:
            enabled: false
          readinessProbe:
            enabled: false
    asserts:
      - hasDocuments:
          count: 2
      - isKind:
          of: Service
      - equal:
          path: metadata.name
          value: RELEASE-NAME-inference-stack-test-vllm
      - equal:
          path: spec.ports[0].port
          value: 8000

  - it: should render Deployment when persistentVolumes enabled
    template: templates/deployment.yaml
    documentSelector:
      path: metadata.name
      value: RELEASE-NAME-inference-stack-test-vllm
    set:
      modelGroups:
        vllm-llama:
          enabled: false
        sglang-qwen:
          enabled: false
        tensorrt-llm-mistral:
          enabled: false
        test-vllm:
          enabled: true
          image:
            repository: nginx
            tag: "latest"
            pullPolicy: IfNotPresent
          persistentVolumes:
            - name: test-data
              size: 100Gi
              storageClass: ""
              accessModes:
                - ReadWriteOnce
              mountPath: /data
          service:
            type: ClusterIP
            port: 8000
          containerPort: 8000
          replicaCount: 1
          modelAlias: ["test"]
          modelName: "test"
          additionalPorts: []
          serviceAccount:
            create: false
          livenessProbe:
            enabled: false
          readinessProbe:
            enabled: false
    asserts:
      - hasDocuments:
          count: 2
      - isKind:
          of: Deployment
      - equal:
          path: metadata.name
          value: RELEASE-NAME-inference-stack-test-vllm

  - it: should render Deployment when persistentVolumes disabled
    template: templates/deployment.yaml
    documentSelector:
      path: metadata.name
      value: RELEASE-NAME-inference-stack-test-vllm
    set:
      modelGroups:
        vllm-llama:
          enabled: false
        sglang-qwen:
          enabled: false
        tensorrt-llm-mistral:
          enabled: false
        test-vllm:
          enabled: true
          image:
            repository: nginx
            tag: "latest"
            pullPolicy: IfNotPresent
          persistentVolumes: []
          service:
            type: ClusterIP
            port: 8000
          containerPort: 8000
          replicaCount: 1
          modelAlias: ["test"]
          modelName: "test"
          additionalPorts: []
          serviceAccount:
            create: false
          livenessProbe:
            enabled: false
          readinessProbe:
            enabled: false
    asserts:
      - hasDocuments:
          count: 2
      - isKind:
          of: Deployment

  - it: should support GPU resources
    template: templates/deployment.yaml
    documentSelector:
      path: metadata.name
      value: RELEASE-NAME-inference-stack-test-vllm
    set:
      modelGroups:
        vllm-llama:
          enabled: false
        sglang-qwen:
          enabled: false
        tensorrt-llm-mistral:
          enabled: false
        test-vllm:
          enabled: true
          image:
            repository: vllm/vllm-openai
            tag: "latest"
            pullPolicy: IfNotPresent
          service:
            type: ClusterIP
            port: 8000
          containerPort: 8000
          replicaCount: 1
          modelAlias:
            - "test-model"
          modelName: "test/model"
          apiKey: ""
          additionalPorts: []
          command: []
          env: []
          livenessProbe:
            enabled: false
          readinessProbe:
            enabled: false
          resources:
            limits:
              nvidia.com/gpu: 2
              memory: 16Gi
            requests:
              nvidia.com/gpu: 1
              memory: 8Gi
          persistentVolumes: []
          podSecurityContext: {}
          securityContext: {}
          serviceAccount:
            create: false
            annotations: {}
          initContainers: []
          volumeMounts: []
          volumes: []
          nodeSelector: {}
          tolerations: []
          affinity: {}
          podAnnotations: {}
          imagePullSecrets: []
    asserts:
      - hasDocuments:
          count: 2
      - equal:
          path: spec.template.spec.containers[0].resources.limits["nvidia.com/gpu"]
          value: 2
      - equal:
          path: spec.template.spec.containers[0].resources.requests["nvidia.com/gpu"]
          value: 1

  - it: should support custom command and env vars
    template: templates/deployment.yaml
    documentSelector:
      path: metadata.name
      value: RELEASE-NAME-inference-stack-test-vllm
    set:
      modelGroups:
        vllm-llama:
          enabled: false
        sglang-qwen:
          enabled: false
        tensorrt-llm-mistral:
          enabled: false
        test-vllm:
          enabled: true
          image:
            repository: vllm/vllm-openai
            tag: "latest"
            pullPolicy: IfNotPresent
          service:
            type: ClusterIP
            port: 8000
          containerPort: 8000
          replicaCount: 1
          modelAlias:
            - "test-model"
          modelName: "test/model"
          apiKey: ""
          additionalPorts: []
          command:
            - "--model"
            - "test-model"
          env:
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
          livenessProbe:
            enabled: false
          readinessProbe:
            enabled: false
          resources: {}
          persistentVolumes: []
          podSecurityContext: {}
          securityContext: {}
          serviceAccount:
            create: false
            annotations: {}
          initContainers: []
          volumeMounts: []
          volumes: []
          nodeSelector: {}
          tolerations: []
          affinity: {}
          podAnnotations: {}
          imagePullSecrets: []
    asserts:
      - hasDocuments:
          count: 2
      - equal:
          path: spec.template.spec.containers[0].command[0]
          value: "--model"
      - equal:
          path: spec.template.spec.containers[0].env[0].name
          value: "CUDA_VISIBLE_DEVICES"