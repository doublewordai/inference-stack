# Example: Multi-Engine Model Deployment
# This example shows how to deploy multiple models using different inference engines
# Demonstrates the full power of the inference-stack with unified API

# Model Groups - Multiple engines serving different use cases
modelGroups:
  # vLLM for general text generation - High throughput, good for chat
  vllm-llama:
    enabled: true
    
    image:
      repository: vllm/vllm-openai
      tag: "v0.6.4"
      pullPolicy: IfNotPresent
    
    modelAlias:
      - "chat"
      - "llama-3.1-8b"
      - "text-generation"
    
    modelName: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    
    service:
      type: ClusterIP
      port: 8000
    
    containerPort: 8000
    replicaCount: 2  # Scale for high availability
    
    # Update strategy for StatefulSet
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        partition: 0  # Update all pods
    
    command:
      - "vllm"
      - "serve"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--model"
      - "meta-llama/Meta-Llama-3.1-8B-Instruct"
      - "--max-model-len"
      - "16384"
      - "--gpu-memory-utilization"
      - "0.85"
      - "--enable-prefix-caching"
      - "--max-num-seqs"
      - "32"
    
    env:
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: HF_HOME
        value: /tmp
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            name: hf-secret
            key: token
    
    startupProbe:
      enabled: true
      type: http
      path: /health
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 60  # 10 minutes for model download
    
    livenessProbe:
      enabled: true
      type: http
      path: /health
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
    
    readinessProbe:
      enabled: true
      type: http
      path: /health
      initialDelaySeconds: 2
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 3
    
    resources:
      requests:
        memory: "16Gi"
        cpu: "4"
        nvidia.com/gpu: "1"
      limits:
        nvidia.com/gpu: "1"
        memory: "24Gi"
    
    # Persistent storage for model cache (StatefulSet volumeClaimTemplates)
    persistentVolumes:
      - name: model-cache
        size: 50Gi
        storageClass: "fast-ssd"
        accessModes:
          - ReadWriteOnce  # Each replica gets its own volume
        mountPath: /root/.cache/huggingface
    
    podSecurityContext:
      runAsUser: 0
    
    nodeSelector:
      nvidia.com/gpu.present: "true"
      node-type: "gpu-standard"
    
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

  # SGLang for embedding models - Optimized for shorter sequences
  sglang-embed:
    enabled: true
    
    image:
      repository: lmsysorg/sglang
      tag: "v0.3.5.post1"
      pullPolicy: IfNotPresent
    
    modelAlias:
      - "embeddings"
      - "embed"
      - "sentence-transformer"
    
    modelName: "BAAI/bge-large-en-v1.5"
    
    service:
      type: ClusterIP
      port: 30001
    
    containerPort: 30001
    replicaCount: 1
    
    # Update strategy for StatefulSet
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        partition: 0  # Update all pods
    
    command:
      - "python"
      - "-m"
      - "sglang.launch_server"
      - "--model-path"
      - "BAAI/bge-large-en-v1.5"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "30001"
      - "--mem-fraction-static"
      - "0.7"
      - "--context-length"
      - "512"  # Shorter for embeddings
      - "--enable-flashinfer"
    
    env:
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: SGLANG_LOG_LEVEL
        value: "info"
    
    startupProbe:
      enabled: true
      type: tcp
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 45  # 7.5 minutes for embedding model download
    
    livenessProbe:
      enabled: true
      type: tcp
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
    
    readinessProbe:
      enabled: true
      type: tcp
      initialDelaySeconds: 2
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 3
    
    resources:
      requests:
        memory: "8Gi"
        cpu: "2"
        nvidia.com/gpu: "1"
      limits:
        nvidia.com/gpu: "1"
        memory: "12Gi"
    
    # Persistent storage for model cache (StatefulSet volumeClaimTemplates)
    persistentVolumes:
      - name: model-cache
        size: 30Gi
        storageClass: "standard"
        accessModes:
          - ReadWriteOnce  # Each replica gets its own volume
        mountPath: /root/.cache
    
    volumes:
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 1Gi
    
    volumeMounts:
      - name: shm
        mountPath: /dev/shm
    
    podSecurityContext:
      runAsUser: 0
    
    nodeSelector:
      nvidia.com/gpu.present: "true"
      node-type: "gpu-standard"

  # TensorRT-LLM for high-performance inference - Optimized compiled models
  tensorrt-codegen:
    enabled: true
    
    image:
      repository: nvcr.io/nvidia/tritonserver
      tag: "24.01-trtllm-python-py3"
      pullPolicy: IfNotPresent
    
    modelAlias:
      - "code-generation"
      - "codegen"
      - "programming"
    
    modelName: "Salesforce/codegen-2B-multi"
    
    service:
      type: ClusterIP
      port: 8001
    
    containerPort: 8001
    replicaCount: 1
    
    # Update strategy for StatefulSet
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        partition: 0  # Update all pods
    
    command:
      - "tritonserver"
      - "--model-repository=/models"
      - "--allow-grpc=false"
      - "--allow-http=true"
      - "--allow-metrics=true"
      - "--http-port=8001"
      - "--metrics-port=8002"
      - "--backend-config=python,shm-default-byte-size=16777216"
    
    additionalPorts:
      - name: metrics
        containerPort: 8002
        servicePort: 8002
    
    env:
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: TENSORRT_LLM_LOG_LEVEL
        value: "INFO"
    
    startupProbe:
      enabled: true
      type: http
      path: /v2/health/ready
      initialDelaySeconds: 60
      periodSeconds: 15
      timeoutSeconds: 10
      failureThreshold: 60  # 15 minutes for TensorRT compilation
    
    livenessProbe:
      enabled: true
      type: http
      path: /v2/health/live
      initialDelaySeconds: 5
      periodSeconds: 15
      timeoutSeconds: 10
      failureThreshold: 3
    
    readinessProbe:
      enabled: true
      type: http
      path: /v2/health/ready
      initialDelaySeconds: 2
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
    
    resources:
      requests:
        memory: "16Gi"
        cpu: "6"
        nvidia.com/gpu: "1"
      limits:
        nvidia.com/gpu: "1"
        memory: "24Gi"
    
    # Persistent storage for compiled models (StatefulSet volumeClaimTemplates)
    persistentVolumes:
      - name: model-storage
        size: 80Gi
        storageClass: "fast-ssd"
        accessModes:
          - ReadWriteOnce  # Each replica gets its own volume
        mountPath: /models
    
    volumes:
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 2Gi
    
    volumeMounts:
      - name: shm
        mountPath: /dev/shm
    
    podSecurityContext:
      runAsUser: 0
    
    nodeSelector:
      nvidia.com/gpu.present: "true"
      node-type: "gpu-high-memory"  # Prefer nodes with more memory
    
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
